{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a1269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re  # Regular expression\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.metrics import confusion_matrix, classification_report,  roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39cddb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMNS=['target','ids','date','flag','user','text']\n",
    "DATASET_ENCODING=\"ISO-8859-1\"\n",
    "df=pd.read_csv(\"twitter_csv.csv\",encoding=DATASET_ENCODING, names=DATASET_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66144147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186342</td>\n",
       "      <td>Fri May 29 07:33:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Madelinedugganx</td>\n",
       "      <td>My GrandMa is making Dinenr with my Mum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186409</td>\n",
       "      <td>Fri May 29 07:33:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>OffRoad_Dude</td>\n",
       "      <td>Mid-morning snack time... A bowl of cheese noo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186429</td>\n",
       "      <td>Fri May 29 07:33:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Falchion</td>\n",
       "      <td>@ShaDeLa same here  say it like from the Termi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186445</td>\n",
       "      <td>Fri May 29 07:33:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jonasobsessedx</td>\n",
       "      <td>@DestinyHope92 im great thaanks  wbuu?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048575</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186607</td>\n",
       "      <td>Fri May 29 07:33:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sugababez</td>\n",
       "      <td>cant wait til her date this weekend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048576 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1048571       4  1960186342  Fri May 29 07:33:44 PDT 2009  NO_QUERY   \n",
       "1048572       4  1960186409  Fri May 29 07:33:43 PDT 2009  NO_QUERY   \n",
       "1048573       4  1960186429  Fri May 29 07:33:44 PDT 2009  NO_QUERY   \n",
       "1048574       4  1960186445  Fri May 29 07:33:44 PDT 2009  NO_QUERY   \n",
       "1048575       4  1960186607  Fri May 29 07:33:45 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1048571  Madelinedugganx           My GrandMa is making Dinenr with my Mum   \n",
       "1048572     OffRoad_Dude  Mid-morning snack time... A bowl of cheese noo...  \n",
       "1048573         Falchion  @ShaDeLa same here  say it like from the Termi...  \n",
       "1048574   jonasobsessedx             @DestinyHope92 im great thaanks  wbuu?  \n",
       "1048575        sugababez               cant wait til her date this weekend   \n",
       "\n",
       "[1048576 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc10b53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "ids       0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea21531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          False\n",
       "1          False\n",
       "2          False\n",
       "3          False\n",
       "4          False\n",
       "           ...  \n",
       "1048571    False\n",
       "1048572    False\n",
       "1048573    False\n",
       "1048574    False\n",
       "1048575    False\n",
       "Length: 1048576, dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6035e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    800000\n",
      "4    248576\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0445b67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Kiran\n",
      "[nltk_data]     Meena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b421b161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"it'll\", 'theirs', \"wasn't\", \"haven't\", 'no', 'shan', 'because', 'him', 'too', 'his', 'so', 'or', 'other', 'should', \"should've\", 'with', 'some', \"shouldn't\", 'll', \"mightn't\", 'until', 'once', 'been', 'further', 'if', 'this', 'will', \"he'd\", 'when', 'how', 'yours', \"they'd\", 'just', 'each', 'in', 'ours', 'me', 'can', \"i've\", \"needn't\", 'nor', 'now', 'their', 'off', \"they're\", 'were', 'which', 'himself', \"won't\", 'whom', 'is', 'both', 'having', \"aren't\", 'did', 'didn', 'myself', 'ma', 'that', 'then', 'isn', 'for', 'own', 'are', 'by', 'over', 'she', 'mightn', 'those', 'weren', 'doesn', \"we've\", \"don't\", 'won', \"hasn't\", \"he'll\", 'here', 'itself', 'about', 'during', 'had', 'he', 'while', 'its', 'more', \"i'm\", 'where', 'them', 'any', \"they'll\", \"he's\", \"i'd\", \"she'd\", 'below', 'wouldn', 'few', 'have', 'i', 'most', 'ain', 'my', 'your', 'yourself', \"she's\", 'all', \"it'd\", 'm', 'of', 'only', 'y', 'hasn', 'we', 'has', 'down', \"weren't\", 'was', 'into', 'same', 'there', 'through', 'as', 'at', 'hadn', \"isn't\", \"didn't\", 'our', 'out', 'the', \"mustn't\", 'after', 'on', \"they've\", 'o', 'than', 'why', 'd', \"you'd\", 'from', 'under', 'wasn', 'these', \"that'll\", 'ourselves', 'above', \"you're\", 'her', 'aren', \"we'd\", 'do', 'between', 'an', 'you', 'very', \"it's\", 're', \"we'll\", 'themselves', 'they', 'mustn', 'up', 'to', \"i'll\", 'again', 's', 'couldn', 'not', 'against', 'a', 'being', 'it', 'what', \"couldn't\", 'who', 'am', 'does', \"shan't\", 'shouldn', 'but', \"you'll\", 't', 'needn', 'haven', 'and', \"she'll\", 'before', \"doesn't\", \"hadn't\", \"you've\", 'such', 'herself', 'don', 'be', 'hers', 've', 'doing', \"wouldn't\", \"we're\", 'yourselves'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96f3de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #\"@user I #love LOVE #AI and machine learning! Check out https://ai.example.com #ArtificalIntelligence\"\n",
    "    \n",
    "    # Function to get NLTK POS tag to WordNet POS tag\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "           return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "           return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "           return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "           return wordnet.ADJ\n",
    "        else:\n",
    "           return wordnet.NOUN\n",
    "     \n",
    "     \n",
    "    \n",
    "    # convert to lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #Remove URLS\n",
    "    text=re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
    "    \n",
    "    #Replace @mentions with 'USER'\n",
    "    text=re.sub(r'@^\\w+','USER',text)\n",
    "    \n",
    "    #REMOVE hashtags but keep the text\n",
    "    text=re.sub(r'#(^\\w+)+','r/1',text)\n",
    "    \n",
    "    #Remove digits\n",
    "    text=re.sub(r'\\d+','',text)\n",
    "    \n",
    "    #Remove extra whitespace\n",
    "    text=re.sub(r'\\s+',' ',text)\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    text=text.strip()\n",
    "    \n",
    "    #Remove stopwords\n",
    "    text=\" \".join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    #Tokenize text\n",
    "    tokenizer=RegexpTokenizer(r'\\w+|[^\\w\\s]')\n",
    "    tokens=tokenizer.tokenize(text)\n",
    "    \n",
    "    #pos position of speech\n",
    "    post_tags=nltk.pos_tag(tokens)\n",
    "    \n",
    "    #Lemmatize the text\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    lemmatized_tokens=[lemmatizer.lemmatize(token,get_wordnet_pos(tag)) for token, tag in post_tags]\n",
    "    \n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94b98c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    800000\n",
      "1    248576\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['target']=df['target'].replace(4,1)\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3ec28d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Kiran Meena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Kiran\n",
      "[nltk_data]     Meena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')  # This will download the tagger in JSON format\n",
    "nltk.download('punkt')  # Tokenizer, if not already downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82e37451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Kiran Meena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4207a65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Kiran\n",
      "[nltk_data]     Meena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cab47bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "166f289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'],df['target'],test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2895386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the IfidVectorizer\n",
    "vectorizer=TfidfVectorizer(max_features=500000,ngram_range=(1,2))\n",
    "\n",
    "# Fit  and transform the training data\n",
    "X_train_vect= vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_vect=vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5edcccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    y_pred = model.predict(X_test_vect)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fc0ffae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90    160127\n",
      "           1       0.73      0.53      0.62     49589\n",
      "\n",
      "    accuracy                           0.84    209716\n",
      "   macro avg       0.80      0.74      0.76    209716\n",
      "weighted avg       0.84      0.84      0.83    209716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Training and evaluating models\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
    "evaluate_model(lr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25b07a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "example_text=\"I #love @kiran\"\n",
    "cleaned_ex_data=clean_text(example_text)\n",
    "vectorized_text=vectorizer.transform([cleaned_ex_data])\n",
    "prediction=lr_model.predict(vectorized_text)\n",
    "print(prediction)\n",
    "                                     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
